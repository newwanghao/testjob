---
title: Ceph社区动态（2021-11-29~2021-12-12）
category: blog 
date: 2021-12-25
tags:
    - Ceph
    - 动态
    - Pacific
sig: ceph-sig
archives: 2021-12
author: rosinL
summary: Ceph社区动态
---
# Ceph社区动态（2021-11-29~2021-12-12）
## Pacific v16.2.7版本发布
2021.12.7，Ceph社区发布了Pacific v16.2.7版本，Pacific版本系列的第7次回合版本，建议所有用户升级到此版本。
本次主要更新：
- 修复OMAP格式升级中的严重缺陷。如果bluestore-quick-fix-on-mount参数设置为true或调用ceph-Bluestore-tool的快速修复命令，
Pacific版本升级后可能会导致数据损坏（格式不正确的OMAP密钥）。相关跟踪：https://tracker.ceph.com/issues/53062。
bluestore-quick-fix-on-mount继续默认设置为false。
- MGR: pg_autoscaler将使用“scale-up”配置文件作为默认配置文件。16.2.6将默认配置文件更改为“scale-down”，
但遇到了device_health_metrics池占用过多PG的问题，这对性能并不理想。因此，在对默认池应消耗的PG数量与“scal-down”配置文件结合
实施限制之前，将默认继续使用“scale-up”配置文件。
- Cephadm和Cephdashboard：NFS管理已完全重新设计，以确保NFS导出在不同Ceph组件中得到一致的管理。
在此之前，有3个不兼容的实现用于配置NFS导出：Ceph-Ansible/ OpenStack Manila、Ceph dashboard和'mgr/nfs'模块。
在此版本中，“mgr/nfs”方式成为官方界面，其余组件（Cephadm和Ceph dashboard）都会遵守它。虽然这可能需要从过时实现手动迁移，
但它将简化那些严重依赖NFS导出的用户体验。
- dashboard：“集群扩展向导”。在“cephdm bootstrap”步骤之后，登录Ceph dashboard的用户将显示欢迎页面。
如果他们选择遵循安装向导，他们将收到一组步骤来帮助他们配置Ceph集群：通过添加更多主机来扩展集群、检测和定义其存储设备、
部署和配置不同的Ceph服务。
- OSD：当使用mclock_shceduler进行QoS时，不再需要运行任何手动基准。OSD现在通过在初始化期间运行简单的基准，
自动为osd_mclock_max_capacity_iops设置适当的值。
- MGR：优化进度模块中的全局恢复事件，在统计信息收集之间增加了5秒的睡眠间隔，以减少进度模块对MGR的影响，
特别是在大型集群中有较好收益。
## 近期社区合入pr
- 在mgr中新增rgw管理模块 [pr#42710](https://github.com/ceph/ceph/pull/42710)
- 支持用perf dump的方式导出bluestore/bluefs的实际分配单元大小 [pr#44098](https://github.com/ceph/ceph/pull/44098)
- 调整了部分bluestore/bluefs内部的计数器的描述，使得描述更加准确，提升可用性 [pr#41557](https://github.com/ceph/ceph/pull/41557)
- 将bluestore_compression_min_blob_size_ssd的大小从8K调整至64K，能够有效减少空间的浪费，并兼顾性能 [pr#39691](https://github.com/ceph/ceph/pull/39691)
- OSD不再支持LevlDB作为KV DB [pr#43612](https://github.com/ceph/ceph/pull/43612)
- 通过自动探测设备的最优io size，并调整min_alloc_size为设备的最优io size。如果设备不支持报告最优的io size，则使用默认的min_alloc_size值 [pr#43691](https://github.com/ceph/ceph/pull/43691)
## 近期Ceph Developer动态
Ceph社区各个模块会定期举行会议，讨论和对齐开发进展，会后有视频上传至[youtube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos)，主要会议信息如下：
|会议名称|说明|频率|
|-------|----|----|
|Crimson SeaStore OSD Weekly Meeting |Crimson & Seastore开发周例会|周|
|Ceph Orchestration Meeting|Ceph管理模块（Mgr）开发|周|
|Ceph DocUBetter Meeting |文档优化|双周|
|Ceph Performance Meeting|Ceph性能优化|双周|
|Ceph Developer Monthly|Ceph开发者月度例会|月|
|Ceph Testing Meeting|版本验证及发布例会|月|
|Ceph Science User Group Meeting|Ceph科学计算领域会议|不定期|

近期主要关注了Developer/Performance/Crimson3个会议，会议讨论了如下内容:
- Developer会议主要介绍了新增的jaeger rgw跟踪功能：能跟踪rgw中每个用户请求，将大对象多次put请求合并到一个对象tag下。
正在进行性能测试：对比编译时开关trace的性能差异、运行时开关trace的性能差异。Jaeger是ceph 16版本引入的新trace，比之前的lttng trace更方便，
不需要额外安装其他依赖包，参见[jaeger文档](https://ceph.com/en/news/blog/2021/distributed-tracing-in-ceph-using-jaeger)
- Performance会议主要讨论了bluestore的共享blob fsck内存使用；通过ceph.conf配置tcmalloc线程缓存值TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES；
glibc malloc导致了内存碎片，即使是tcmalloc也有待进一步优化。
- Crimson会议同步了SeaStore的开发进展、问题: 完善crimson OSD，实现了经典OSD已有的功能。当ceph.conf没有配置ip时，OSD从mon获取公共ip地址用于心跳。
讨论了LBA CPU开销时间长的问题，修改策略待定；GC占用了CPU从而导致了整体性能下降，需要提升GC性能。
