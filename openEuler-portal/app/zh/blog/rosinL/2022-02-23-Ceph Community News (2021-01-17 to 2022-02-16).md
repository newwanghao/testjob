---
title: Ceph社区动态（2022-1-17~2022-2-16）
category: blog 
date: 2022-2-23
tags:
    - Ceph
    - 动态
    - Pacific
sig: ceph-sig
archives: 2022-2
author: rosinL
summary: Ceph社区动态
---
# Ceph社区动态（2021-1-17~2022-2-16）
## Cephalocon 2022推迟举办
由于COVID-19大流行，原定于美国东部时间4月5-7日（北京时间4月6-8日）举办的Cephalocon 2022推迟举办，修订时间待定。本次Cephalocon 2022会议有关议题已经公布，整理如下，详情请参考[Cephalocon 2022 sched](https://ceph2022.sched.com/)
|分类|议题|演讲者|机构|
|----|-------|----|----|
|RGW, Performance|[Optimizing RGW Object Storage Mixed Media through Storage Classes and Lua Scripting](https://sched.co/w9FL)|Curt Bruns & Anthony D'Atri|Intel|
|RGW|[RGW: Sync What? Sync Info Provider: Early Peek](https://sched.co/w9Fm)|Yehuda Sadeh-Weinraub|Red Hat|
|RGW|[RGW – An Ultimate S3 Frontend for MultipleBackends: An Implementation Story](https://sched.co/w9GJ)|Gregory Touretsky & Andriy Tkachuk|Seagate|
|RGW, S3select|[S3select: Computational Storage in S3](https://sched.co/w9GY)|Gal Salomon & Girjesh Rajoria|Red Hat|
|RGW|[Testing S3 Implementations: RGW & Beyond](https://sched.co/w9Gh)|Robin Hugh Johnson|DigitalOcean|
|RGW|[Introduction to Container Object Storage Interface aka COSI for ceph RGW](https://sched.co/w9Fs)|Jiffin Tony Thottan|Red Hat|
|RGW|[RGW Zipper](https://sched.co/w9GD)|Daniel Gryniewicz & Soumya Koduri|Red Hat|
|Cephadm|[Lightning Talk: Introduction to Cephadm](https://sched.co/w9EW)|Melissa Li|Red Hat|
|Dashboard|[Dashboard: Exploring Centralized Logging with Ceph Storage](https://sched.co/w9GP)|Gaurav Sitlani & Aashish Sharma|Red Hat|
|Dashboard|[Operating Ceph from the Ceph Dashboard: Past, Present and Future](https://sched.co/w9F0)|Ernesto Puerta|Red Hat|
|Ceph, QoS, mClock|[Ceph QoS Refinements for Background Operations using mClock](https://sched.co/w9Fv)|Sridhar Seshasayee|Red Hat|
|Ceph, PG|[pgremapper: CRUSHing Cluster OperationaComplexity](https://sched.co/w9EZ)|Joshua Baergen|DigitalOcean|
|Ceph, PG|[New Workload Balancer in Ceph](https://sched.co/w9Eo)|Josh Salomon & Laura Flores|Red Hat|
|Ceph, DPDK|[Lightning Talk: Ceph Messenger DPDkstack Development and Debugging](https://sched.co/w9FO)|Chunsong Feng|Huawei|
|Ceph, Windows|[Ceph on Windows](https://sched.co/w9Ei)|Alessandro Pilotti|Cloudbase Solutions|
|Seastore|[What's New with Crimson and Seastore?](https://sched.co/w9FI)|Samuel Just|Red Hat|
|Seastore, Crimson|[Lightning Talk: Introduction to Crimson from a Newbie](https://sched.co/w9FF)|Joseph Sawaya|Red Hat|
|Seastore|[Understanding SeaStore Through Profiling](https://sched.co/w9ET)|Yingxin Cheng & Tushar Gohad|Intel|
|Bluestore|[Accelerating PMEM Device Operations in BlueStore with Hardware Based Memory Offloading Technique](https://sched.co/w9F9)|Ziye Yang|Intel|
|Bluestore|[Revealing BlueStore Corruption Bugs in Containerized Ceph Clusters](https://sched.co/w9Fj)|Satoru Takeuchi|Cybozu|
|Dev|[Chasing Bad Checksums: A Journey through Ceph, TCMalloc, and the Linux kernel](https://sched.co/w9Fd)|Mauricio Faria de Oliveira & Dan Hill|Canonical|
|Dev|[Lightning Talk: Improving Ceph Build and Backport Automations Using Github Actions](https://sched.co/w9Gt)|Deepika Upadhyay|Red Hat|
|Dev|[Ceph Crash Telemetry Observability in Action](https://sched.co/w9Ec)|Yaarit Hatuka|Red Hat|
|Performance|[DisTRaC: Accelerating High-Performance Compute Processing for Temporary Data Storage](https://sched.co/w9Ef)|Gabryel Mason-Williams|Rosalind Franklin Institute|
|Performance|[Putting the Compute in your Storage](https://sched.co/w9Fg)|Federico Lucifredi & Brad Hubbard|Red Hat|
|Performance|[Modifying Ceph for Better HPC Performance](https://sched.co/w9Gb)|Darren Soothill|CROIT|
|Performance|[Over A Billion Requests Served Per Day: Ensuring Everyone is Happy with Our Ceph Clusters’ Performance](https://sched.co/w9FR)|Jane Zhu & Matthew Leonard|Bloomberg LP|
|Performance|[Lessons Learned from Hardware Acceleration Initiatives for Ceph-specific Workloads](https://sched.co/w9G4)|Harry Richardson & Lionel Corbet|SoftIron|
|Performance|[The Effort to Exploit Modern SSDs on Ceph](https://sched.co/w9GG)|Myoungwon Oh|Samsung Electronics|
|Performance|[NVMe-over-Fabrics Support for Ceph](https://sched.co/w9GS)|Jonas Pfefferle, IBM & Scott Peterson|Intel|
|Security|[Introducing the New RBD Image Encryption Feature](https://sched.co/w9F3)|Or Ozeri & Danny Harnik|IBM|
|Security|[CephFS At-Rest Encryption with fscrypt](https://sched.co/w9Eu)|Jeffrey Layton|Red Hat|
|Security|[Secure Token Service in Ceph](https://sched.co/w9Ex)|Pritha Srivastava|Red Hat|
|Security|[Data Security and Storage Hardening in Rook and Ceph](https://sched.co/w9Fp)|Federico Lucifred & Michael Hackett|Red Hat|
|Ceph应用|[Improved Business Continuity for an Existing Large Scale Ceph Infrastructure: A Story from Practical Experience](https://sched.co/w9G7)|Enrico Bocch & Arthur Outhenin-Chalandre|CERN|
|Ceph应用|[How we Operate Ceph at Scale](https://sched.co/w9Fy)|Matt Vandermeulen|Digital Ocean|
|Ceph应用|[BoF Session: Ceph in Scientific Computing and Large Clusters](https://sched.co/w9FC)|Kevin Hrpcek|Space Science & Engineering Center, University of Wisconsin - Madison|
|Ceph应用|[Aquarium: An Easy to Use Ceph Appliance](https://sched.co/w9Ge)|Joao Eduardo Luis & Alexandra Settle|SUSE|
|Ceph应用|[Stretch Clusters in Ceph: Algorithms, Use Cases, and Improvements](https://sched.co/w9Gn)|Gregory Farnum|Red Hat|
|Ceph应用|[We Added 6 Petabytes of Ceph Storage and No Clients Noticed! Here’s How We Did It.](https://sched.co/w9FX)|Joseph Mundackal & Matthew Leonard|Bloomberg LP|
|Ceph应用|[Why We Built A “Message-Driven Telemetry System At Scale” Ceph Cluster](https://sched.co/w9FU)|Xiaolin Lin & Matthew Leonard|Bloomberg LP|
|Ceph应用|[Lightning Talk: Ceph and 6G: Are We Ready for zettabytes?](https://sched.co/w9Gk)|Babar Khan|Technical University Darmstadt|
|Ceph应用|[Bringing emails@ceph Into the Field](https://sched.co/w9G1)|Danny Al-Gaaf|Deutsche Telekom AG|
|Ceph应用|[Lightning Talk: Ceph and QCOW2 a Match Made in Heaven: From Live Migration to Differential Snapshots](https://sched.co/w9F6)|Effi Ofer|IBM|
|Ceph应用|[Lightning Talk: Installing Ceph on Kubernetes Using the Rook Operator and Helm](https://sched.co/w9GM)|Mike Petersen|Platform9|
|Benchmark|[Connecting The Dots: Benchmarking Ceph at Scale](https://sched.co/w9GA)|Shon Paz & Ido Pal|Red Hat|
|Benchmark|[Introducing Sibench: A New Open Source Benchmarking Optimized for Ceph](https://sched.co/w9GV)|Danny Abukalam|SoftIron|
## 近期社区合入pr
近期pr主要以bug修复为主，摘选了部分如下：
- mgr, 当osd in/out时，默认关闭pg recovery，需要时手动开启，以降低对服务集群的影响 [pr#44588](https://github.com/ceph/ceph/pull/44588)
- osd, 增加ceph daemon perf dump中dump_blocked_ops_count选项，来快速获取blocked ops 个数，以解决原来的dump_blocked_ops操作带来的较大开销 [pr#44780](https://github.com/ceph/ceph/pull/44780)
- rgw, rgw s3 CopyObject接口支持条件拷贝 [pr#44678](https://github.com/ceph/ceph/pull/44678)
- rgw, 修复radosgw-admin bucket chown过程中大量内存使用的问题 [pr#44357](https://github.com/ceph/ceph/pull/44357)
- rbd, krbd中引入rxbounce选项，解决image作为windows系统块设备时，crc校验出错和带来性能下降的问题 [pr#44842](https://github.com/ceph/ceph/pull/44842)
## 近期Ceph Developer动态
Ceph社区各个模块会定期举行会议，讨论和对齐开发进展，会后有视频上传至[youtube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos)，主要会议信息如下：
|会议名称|说明|频率|
|-------|----|----|
|Crimson SeaStore OSD Weekly Meeting |Crimson & Seastore开发周例会|周|
|Ceph Orchestration Meeting|Ceph管理模块（Mgr）开发|周|
|Ceph DocUBetter Meeting |文档优化|双周|
|Ceph Performance Meeting|Ceph性能优化|双周|
|Ceph Developer Monthly|Ceph开发者月度例会|月|
|Ceph Testing Meeting|版本验证及发布例会|月|
|Ceph Science User Group Meeting|Ceph科学计算领域会议|不定期|
|Ceph Leadership Team Meeting|Ceph领导团队例会|周|

近期社区重点投入Quincy版本的冻结测试与验证，提取关键内容:
- Quincy测试，读性能符合预期，写性能部分场景有下降，但可以确定4k min_alloc_size，bluestore allocator优化有助于性能提升。
- 随着omap规模增长，omap_iterator效率会导致大量的slow_ops甚至拒绝响应。[issue](https://tracker.ceph.com/issues/53926)记录了两种压缩方式测试结果：手动触发压缩，时延无法恢复至压缩前的水平；Rocksdb提供的周期性[ttl压缩功能](https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide#periodic-and-ttl-compaction)，使能后，时延可以恢复至压缩前水平。
- CBT(Ceph Benchmark Tool)工具新增大量PR合入，重点关注大规模osd测试时对mem资源的控制，以及多client并发测试用例的测试。
